diff --git a/.gitignore b/.gitignore
index 2e86fdf..53406fc 100644
--- a/.gitignore
+++ b/.gitignore
@@ -2,3 +2,4 @@
 Megatron-LM-*
 LM-Evaluation-Harness*
 Bigcode-Evaluation-Harness*
+*__pycache__*
Submodule LM-Evaluation-Harness-240310 contains modified content
diff --git a/LM-Evaluation-Harness-240310/lm_eval/__main__.py b/LM-Evaluation-Harness-240310/lm_eval/__main__.py
index b9dd4157..8a7188d6 100644
--- a/LM-Evaluation-Harness-240310/lm_eval/__main__.py
+++ b/LM-Evaluation-Harness-240310/lm_eval/__main__.py
@@ -17,6 +17,23 @@ from lm_eval.tasks import TaskManager, include_path, initialize_tasks
 from lm_eval.utils import make_table, simple_parse_args_string
 
 
+import torch
+from functools import wraps
+def fix_int64(func):
+    @wraps(func)
+    def wrapper(*args, **kwargs):
+        dtype = kwargs.get('dtype')
+        if dtype is not None:
+            if isinstance(dtype, str):
+                dtype = getattr(torch, dtype, dtype)
+            if dtype == torch.int64:
+                kwargs['dtype'] = torch.int32
+        return func(*args, **kwargs)
+    return wrapper
+
+torch.empty = fix_int64(torch.empty)
+torch.arange = fix_int64(torch.arange)
+
 DEFAULT_RESULTS_FILE = "results.json"
 
 
diff --git a/LM-Evaluation-Harness-240310/lm_eval/models/huggingface.py b/LM-Evaluation-Harness-240310/lm_eval/models/huggingface.py
index fc78b55a..f80fd7e6 100644
--- a/LM-Evaluation-Harness-240310/lm_eval/models/huggingface.py
+++ b/LM-Evaluation-Harness-240310/lm_eval/models/huggingface.py
@@ -34,6 +34,43 @@ from lm_eval.models.utils import (
     stop_sequences_criteria,
 )
 
+from torch_tpu.tpu.custom_op.llama_attn_qkv import fuse_qwen2_attn_qkv
+from torch_tpu.tpu.custom_op.rmsnorm import fuse_qwen2_rmsnorm
+from torch_tpu.tpu.custom_op.llama_mlp import fuse_qwen2_mlp
+#replace transformers func because lacking tril op
+from typing import Optional
+def make_causal_mask(
+    input_ids_shape: torch.Size,
+    dtype: torch.dtype,
+    device: torch.device,
+    past_key_values_length: int = 0,
+    sliding_window: Optional[int] = None,
+):
+    """
+    Make causal mask used for bi-directional self-attention.
+    """
+    bsz, tgt_len = input_ids_shape
+    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)
+    mask_cond = torch.arange(mask.size(-1), device=device)
+    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
+
+    mask = mask.to(dtype)
+
+    if past_key_values_length > 0:
+        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)
+
+    # add lower triangular sliding window mask if necessary
+    if sliding_window is not None:
+        diagonal = past_key_values_length - sliding_window - 1
+
+        i = torch.arange(tgt_len, dtype=torch.int32, device=device).unsqueeze(1)
+        j = torch.arange(tgt_len, dtype=torch.int32, device=device).unsqueeze(0)
+        condition = j < torch.minimum(i + diagonal + 1, torch.tensor(tgt_len, device=device))
+        mask.masked_fill_(condition, torch.finfo(dtype).min)
+
+    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length).contiguous()
+import transformers
+transformers.modeling_attn_mask_utils.AttentionMaskConverter._make_causal_mask = staticmethod(make_causal_mask)
 
 eval_logger = utils.eval_logger
 
@@ -112,7 +149,9 @@ class HFLM(TemplateLM):
         **kwargs,
     ) -> None:
         super().__init__()
-
+        fuse_qwen2_attn_qkv()
+        fuse_qwen2_rmsnorm()
+        fuse_qwen2_mlp()
         # optionally: take in an already-initialized transformers.PreTrainedModel
         if not isinstance(pretrained, str):
             eval_logger.warning(
@@ -153,8 +192,9 @@ class HFLM(TemplateLM):
             if not (parallelize or accelerator.num_processes > 1):
                 # use user-passed device
                 device_list = set(
-                    ["cuda", "cpu"]
+                    ["cuda", "cpu", "tpu"]
                     + [f"cuda:{i}" for i in range(torch.cuda.device_count())]
+                    + [f"tpu:{i}" for i in range(torch.tpu.device_count())]
                     + ["mps", "mps:0"]
                 )
                 if device and device in device_list:
@@ -168,10 +208,10 @@ class HFLM(TemplateLM):
                         )
                 else:
                     eval_logger.info("Device not specified")
-                    eval_logger.info(f"Cuda Available? {torch.cuda.is_available()}")
+                    eval_logger.info(f"TPU Available? {torch.tpu.is_available()}")
                     self._device = (
-                        torch.device("cuda")
-                        if torch.cuda.is_available()
+                        torch.device("tpu")
+                        if torch.tpu.is_available()
                         else torch.device("cpu")
                     )
             else:
@@ -957,14 +997,14 @@ class HFLM(TemplateLM):
                 if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:
                     inp = torch.tensor(
                         (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1],
-                        dtype=torch.long,
+                        dtype=torch.int32,
                         device=self.device,
                     )
                     (inplen,) = inp.shape
                 elif self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:
                     inp = torch.tensor(
                         (context_enc)[-self.max_length :],
-                        dtype=torch.long,
+                        dtype=torch.int32,
                         device=self.device,
                     )
                     (inplen,) = inp.shape
@@ -976,7 +1016,7 @@ class HFLM(TemplateLM):
                         (continuation_enc)[-self.max_length :],
                         # TODO: left-shift these?
                         # TODO: our code assumes we never end up truncating conts for either model type
-                        dtype=torch.long,
+                        dtype=torch.int32,
                         device=self.device,
                     )
                     (contlen,) = cont.shape
@@ -1025,6 +1065,7 @@ class HFLM(TemplateLM):
                 self._model_call(batched_inps, **call_kwargs), dim=-1
             )  # [batch, padding_length (inp or cont), vocab]
 
+            multi_logits = multi_logits.to("cpu")
             for (request_str, ctx_tokens, _), logits, inplen, cont_toks in zip(
                 chunk, multi_logits, inplens, cont_toks_list
             ):
@@ -1057,7 +1098,7 @@ class HFLM(TemplateLM):
                     logits=logits,
                 ):
                     cont_toks = torch.tensor(
-                        cont_toks, dtype=torch.long, device=self.device
+                        cont_toks, dtype=torch.int64
                     ).unsqueeze(0)  # [1, seq]
                     max_equal = (greedy_tokens == cont_toks).all()
 
diff --git a/LM-Evaluation-Harness-240310/lm_eval/models/utils.py b/LM-Evaluation-Harness-240310/lm_eval/models/utils.py
index 09818f4e..1419a577 100644
--- a/LM-Evaluation-Harness-240310/lm_eval/models/utils.py
+++ b/LM-Evaluation-Harness-240310/lm_eval/models/utils.py
@@ -165,7 +165,7 @@ def pad_and_concat(
                         tensor,  # [seq]
                         torch.zeros(
                             max_length - tensor_len,
-                            dtype=torch.long,
+                            dtype=torch.int32,
                             device=tensor.device,
                         ),  # [padding_length - seq]
                     ],
@@ -177,7 +177,7 @@ def pad_and_concat(
                     [
                         torch.zeros(
                             max_length - tensor_len,
-                            dtype=torch.long,
+                            dtype=torch.int32,
                             device=tensor.device,
                         ),  # [padding_length - seq]
                         tensor,  # [seq]
Submodule PAI-Megatron-LM-240718 contains modified content
diff --git a/PAI-Megatron-LM-240718/megatron/core/distributed/distributed_data_parallel.py b/PAI-Megatron-LM-240718/megatron/core/distributed/distributed_data_parallel.py
index 7b95b858..c863eaa0 100644
--- a/PAI-Megatron-LM-240718/megatron/core/distributed/distributed_data_parallel.py
+++ b/PAI-Megatron-LM-240718/megatron/core/distributed/distributed_data_parallel.py
@@ -196,6 +196,7 @@ class DistributedDataParallel(MegatronModule):
                 grad_acc = param_tmp.grad_fn.next_functions[0][0]
                 grad_acc.register_hook(self._make_param_hook(param, self.param_to_buffer))
                 self.grad_accs.append(grad_acc)
+                param_tmp.view(-1)[-1].cpu() # force sync; workaround for memory access violation between d2d and h2d
 
     def forward(self, *inputs, **kwargs):
         """
diff --git a/PAI-Megatron-LM-240718/megatron/core/distributed/param_and_grad_buffer.py b/PAI-Megatron-LM-240718/megatron/core/distributed/param_and_grad_buffer.py
index efed47c5..eaa2dde4 100644
--- a/PAI-Megatron-LM-240718/megatron/core/distributed/param_and_grad_buffer.py
+++ b/PAI-Megatron-LM-240718/megatron/core/distributed/param_and_grad_buffer.py
@@ -363,13 +363,17 @@ class ParamAndGradBuffer:
                 device=torch.cuda.current_device(),
                 requires_grad=False,
             )
-        self.grad_data = torch.zeros(
+        self.grad_data = torch.empty(
             self.numel,
             dtype=self.grad_dtype,
             device=torch.cuda.current_device(),
             requires_grad=False,
         )
 
+        chunk_size = 1073741824
+        for i in range(0, self.numel, chunk_size):
+            self.grad_data[i: i + chunk_size].zero_()
+
         # Finally, map param.data and param.main_grad fields to buffers.
         bucket_params = set()
         bucket_data_start_index = 0
@@ -436,7 +440,11 @@ class ParamAndGradBuffer:
 
     def scale_gradients(self, scaling_factor: float) -> None:
         """Scale the gradient data by `scaling_factor`."""
-        self.grad_data *= scaling_factor
+        # self.grad_data *= scaling_factor
+        # a workaround for tensor shape ge 2^31
+        chunk_size = 1073741824
+        for i in range(0, self.numel, chunk_size):
+            self.grad_data[i: i + chunk_size] *= scaling_factor
 
     def _get(self, shape: torch.Size, start_index: int, buffer_type: BufferType) -> torch.Tensor:
         """
@@ -505,7 +513,12 @@ class ParamAndGradBuffer:
         Zero out the underlying grad_buffer and reset all buckets in preparation for the next
         iteration of training.
         """
-        self.grad_data.zero_()
+        # self.grad_data.zero_()
+        # a workaround for tensor shape ge 2^31
+        chunk_size = 1073741824
+        for i in range(0, self.numel, chunk_size):
+            self.grad_data[i: i + chunk_size].zero_()
+
         for bucket in self.buckets:
             bucket.reset()
         self.is_last_microbatch = True
diff --git a/PAI-Megatron-LM-240718/megatron/core/models/common/embeddings/language_model_embedding.py b/PAI-Megatron-LM-240718/megatron/core/models/common/embeddings/language_model_embedding.py
index bc1a2de9..940855ef 100644
--- a/PAI-Megatron-LM-240718/megatron/core/models/common/embeddings/language_model_embedding.py
+++ b/PAI-Megatron-LM-240718/megatron/core/models/common/embeddings/language_model_embedding.py
@@ -9,6 +9,8 @@ from megatron.core import tensor_parallel
 from megatron.core.transformer.module import MegatronModule
 from megatron.core.transformer.transformer_config import TransformerConfig
 
+import os
+from distutils.util import strtobool
 
 class LanguageModelEmbedding(MegatronModule):
     """Language model embeddings.
@@ -106,7 +108,9 @@ class LanguageModelEmbedding(MegatronModule):
 
         if not self.reduce_scatter_embeddings:
             # Data format change to avoid explicit tranposes : [b s h] --> [s b h].
-            embeddings = embeddings.transpose(0, 1).contiguous()
+            whole_net_trans = strtobool(os.environ.get("QWEN2_WHOLE_NET_TRANS", "0"))
+            if not whole_net_trans:
+                embeddings = embeddings.transpose(0, 1).contiguous()
 
         if tokentype_ids is not None:
             assert self.tokentype_embeddings is not None
diff --git a/PAI-Megatron-LM-240718/megatron/core/models/common/embeddings/rotary_pos_embedding.py b/PAI-Megatron-LM-240718/megatron/core/models/common/embeddings/rotary_pos_embedding.py
index d4e6be8c..6454d745 100644
--- a/PAI-Megatron-LM-240718/megatron/core/models/common/embeddings/rotary_pos_embedding.py
+++ b/PAI-Megatron-LM-240718/megatron/core/models/common/embeddings/rotary_pos_embedding.py
@@ -3,6 +3,8 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Optional
+import os
+from distutils.util import strtobool
 
 if TYPE_CHECKING:
     from megatron.core.transformer.transformer_config import TransformerConfig
@@ -138,10 +140,11 @@ class RotaryEmbedding(nn.Module):
         if inference_params is not None:
             rotary_seq_len = inference_params.max_sequence_length
         else:
+            whole_net_trans = strtobool(os.environ.get("QWEN2_WHOLE_NET_TRANS", "0"))
             if transformer.input_tensor is not None:
-                rotary_seq_len = transformer.input_tensor.size(0)
+                rotary_seq_len = transformer.input_tensor.size(0 + int(whole_net_trans))
             else:
-                rotary_seq_len = transformer_input.size(0)
+                rotary_seq_len = transformer_input.size(0 + int(whole_net_trans))
 
             if transformer_config.sequence_parallel:
                 rotary_seq_len *= transformer_config.tensor_model_parallel_size
diff --git a/PAI-Megatron-LM-240718/megatron/core/models/common/language_module/language_module.py b/PAI-Megatron-LM-240718/megatron/core/models/common/language_module/language_module.py
index cd9b14df..6e8c48c9 100644
--- a/PAI-Megatron-LM-240718/megatron/core/models/common/language_module/language_module.py
+++ b/PAI-Megatron-LM-240718/megatron/core/models/common/language_module/language_module.py
@@ -11,6 +11,8 @@ from megatron.core.transformer.module import MegatronModule
 from megatron.core.transformer.transformer_config import TransformerConfig
 from megatron.core.utils import make_tp_sharded_tensor_for_checkpoint
 
+import os
+from distutils.util import strtobool
 
 class LanguageModule(MegatronModule):
     """Base language module that has common helper functions used across GPT, BERT etc.
@@ -33,14 +35,17 @@ class LanguageModule(MegatronModule):
             Tensor: Loss tensor of dimensions [batch size, sequence_length]
         """
         # [b s] => [s b]
-        labels = labels.transpose(0, 1).contiguous()
+        whole_net_trans = strtobool(os.environ.get("QWEN2_WHOLE_NET_TRANS", "0"))
+        if not whole_net_trans:
+            labels = labels.transpose(0, 1).contiguous()
         if self.config.cross_entropy_loss_fusion:
             loss = fused_vocab_parallel_cross_entropy(logits, labels)
         else:
             loss = tensor_parallel.vocab_parallel_cross_entropy(logits, labels)
 
         # [s b] => [b, s]
-        loss = loss.transpose(0, 1).contiguous()
+        if not whole_net_trans:
+            loss = loss.transpose(0, 1).contiguous()
         return loss
 
     def setup_embeddings_and_output_layer(self) -> None:
diff --git a/PAI-Megatron-LM-240718/megatron/core/optimizer/__init__.py b/PAI-Megatron-LM-240718/megatron/core/optimizer/__init__.py
index 8552ab8f..97ea1814 100644
--- a/PAI-Megatron-LM-240718/megatron/core/optimizer/__init__.py
+++ b/PAI-Megatron-LM-240718/megatron/core/optimizer/__init__.py
@@ -9,7 +9,9 @@ try:
     from transformer_engine.pytorch.optimizers import FusedSGD as SGD
 except ImportError:
     try:
-        from apex.optimizers import FusedAdam as Adam
+        from functools import partial
+        from apex.optimizers import FusedAdam
+        Adam = partial(FusedAdam, adam_w_mode=False)
         from apex.optimizers import FusedSGD as SGD
     except ImportError:
         import warnings
@@ -20,7 +22,7 @@ except ImportError:
 
         ## apex's FusedAdam is a drop-in replacement for torch's AdamW
         ## see https://github.com/NVIDIA/apex/blob/7b73b12361068a10b0f44844534613f252a5ea75/apex/optimizers/fused_adam.py#L16
-        from torch.optim import AdamW as Adam, SGD
+        from torch.optim import Adam, SGD
 
 from megatron.core import mpu
 
diff --git a/PAI-Megatron-LM-240718/megatron/core/optimizer/clip_grads.py b/PAI-Megatron-LM-240718/megatron/core/optimizer/clip_grads.py
index 708ccd01..b3edc26a 100644
--- a/PAI-Megatron-LM-240718/megatron/core/optimizer/clip_grads.py
+++ b/PAI-Megatron-LM-240718/megatron/core/optimizer/clip_grads.py
@@ -139,7 +139,7 @@ def clip_grad_by_total_norm_fp32(
     grads = []
     for param in parameters:
         if param.grad is not None:
-            assert param.grad.type() == 'torch.cuda.FloatTensor'
+            assert param.grad.type() in ['torch.cuda.FloatTensor', 'torch.tpu.FloatTensor']
             grads.append(param.grad.detach())
 
     # Scale.
diff --git a/PAI-Megatron-LM-240718/megatron/core/optimizer/optimizer.py b/PAI-Megatron-LM-240718/megatron/core/optimizer/optimizer.py
index 989a4438..238e59e8 100644
--- a/PAI-Megatron-LM-240718/megatron/core/optimizer/optimizer.py
+++ b/PAI-Megatron-LM-240718/megatron/core/optimizer/optimizer.py
@@ -502,7 +502,7 @@ class Float16OptimizerWithFloat16Params(MixedPrecisionOptimizer):
                 if param.requires_grad:
 
                     # float16 params:
-                    if param.type() in ['torch.cuda.HalfTensor', 'torch.cuda.BFloat16Tensor']:
+                    if param.type() in ['torch.cuda.HalfTensor', 'torch.cuda.BFloat16Tensor', 'torch.tpu.HalfTensor', 'torch.tpu.BFloat16Tensor']:
                         float16_params_this_group.append(param)
                         # Create a copy
                         main_param = param.detach().clone().float()
@@ -518,7 +518,7 @@ class Float16OptimizerWithFloat16Params(MixedPrecisionOptimizer):
                         if param in self.optimizer.state:
                             self.optimizer.state[main_param] = self.optimizer.state.pop(param)
                     # fp32 params.
-                    elif param.type() == 'torch.cuda.FloatTensor':
+                    elif param.type() in ['torch.cuda.FloatTensor', 'torch.tpu.FloatTensor']:
                         fp32_params_this_group.append(param)
                         param_group['params'][i] = param
 
@@ -553,10 +553,13 @@ class Float16OptimizerWithFloat16Params(MixedPrecisionOptimizer):
         main_grads = []
 
         # fp32 params from float16 ones.
-        for main_group in self.fp32_from_float16_groups:
-            for main_param in main_group:
-                if main_param.grad is not None:
-                    main_grads.append(main_param.grad.data)
+        # for main_group in self.fp32_from_float16_groups:
+        #     for main_param in main_group:
+        #         if main_param.grad is not None:
+        #             main_grads.append(main_param.grad.data)
+        chunk_size = 1073741824
+        for i in range(0, self._all_grad_data_fp32.numel(), chunk_size):
+            main_grads.append(self._all_grad_data_fp32[i : i + chunk_size])
 
         # Append fp32 parameters.
         for main_group in self.fp32_from_fp32_groups:
@@ -576,14 +579,36 @@ class Float16OptimizerWithFloat16Params(MixedPrecisionOptimizer):
         return model_data, main_data
 
     def _copy_model_grads_to_main_grads(self):
+        cnt = 0
+        for model_group, main_group in zip(self.float16_groups, self.fp32_from_float16_groups):
+            for model_param, main_param in zip(model_group, main_group):
+                if hasattr(model_param, 'main_grad'):
+                    cnt += model_param.main_grad.numel()
+                else:
+                    cnt += model_param.grad.numel()
+
+        if not hasattr(self, '_all_grad_data_fp32'):
+            self._all_grad_data_fp32 = torch.empty(cnt, dtype=torch.float, device='cuda')
+        else:
+            # check if the size is enough
+            if self._all_grad_data_fp32.numel() < cnt:
+                self._all_grad_data_fp32 = torch.empty(cnt, dtype=torch.float, device='cuda')
+
+        offset = 0
         # This only needs to be done for the float16 group.
         for model_group, main_group in zip(self.float16_groups, self.fp32_from_float16_groups):
             for model_param, main_param in zip(model_group, main_group):
                 if hasattr(model_param, 'main_grad'):
-                    main_param.grad = model_param.main_grad.float()
+                    # main_param.grad = model_param.main_grad.float()
+                    main_param.grad = self._all_grad_data_fp32[offset : offset + model_param.main_grad.numel()].view(model_param.main_grad.size())
+                    offset += model_param.main_grad.numel()
+                    main_param.grad.copy_(model_param.main_grad) # force dtype convert
                 else:
                     if model_param.grad is not None:
-                        main_param.grad = model_param.grad.float()
+                        # main_param.grad = model_param.grad.float()
+                        main_param.grad = self._all_grad_data_fp32[offset : offset + model_param.grad.numel()].view(model_param.grad.size())
+                        offset += model_param.grad.numel()
+                        main_param.grad.copy_(model_param.grad) # force dtype convert
 
                 # Safe to deallocate model's grad/main_grad after copying.
                 # (If using contiguous buffers, main_grad's memory should
diff --git a/PAI-Megatron-LM-240718/megatron/core/tensor_parallel/cross_entropy.py b/PAI-Megatron-LM-240718/megatron/core/tensor_parallel/cross_entropy.py
index 45fa0751..8524b795 100644
--- a/PAI-Megatron-LM-240718/megatron/core/tensor_parallel/cross_entropy.py
+++ b/PAI-Megatron-LM-240718/megatron/core/tensor_parallel/cross_entropy.py
@@ -53,9 +53,12 @@ class VocabParallelCrossEntropy:
         partition_vocab_size = vocab_parallel_logits.size()[-1]
         logits_2d = vocab_parallel_logits.view(-1, partition_vocab_size)
         masked_target_1d = masked_target.view(-1)
-        arange_1d = torch.arange(start=0, end=logits_2d.size()[0], device=logits_2d.device)
-        predicted_logits_1d = logits_2d[arange_1d, masked_target_1d]
-        predicted_logits_1d = predicted_logits_1d.clone().contiguous()
+        # arange_1d = torch.arange(start=0, end=logits_2d.size()[0], device=logits_2d.device)
+        # predicted_logits_1d = logits_2d[arange_1d, masked_target_1d]
+        # predicted_logits_1d = predicted_logits_1d.clone().contiguous()
+        predicted_logits_1d = torch.empty(masked_target_1d.shape[0], 1, dtype = logits_2d.dtype, device = logits_2d.device)
+        torch.gather(logits_2d, 1, masked_target_1d.unsqueeze(1), out=predicted_logits_1d)
+        predicted_logits_1d = predicted_logits_1d.squeeze(1)
         predicted_logits = predicted_logits_1d.view_as(target)
         predicted_logits[target_mask] = 0.0
 
@@ -107,7 +110,8 @@ class VocabParallelCrossEntropy:
         grad_output: torch.Tensor,
     ) -> torch.Tensor:
 
-        grad_2d[arange_1d, masked_target_1d] -= softmax_update
+        #grad_2d[arange_1d, masked_target_1d] -= softmax_update
+        grad_2d.scatter_add_(dim=1, index=masked_target_1d.unsqueeze(1), src= -softmax_update.unsqueeze(1))
 
         # Finally elementwise multiplication with the output gradients.
         grad_input.mul_(grad_output.unsqueeze(dim=-1))
diff --git a/PAI-Megatron-LM-240718/megatron/core/utils.py b/PAI-Megatron-LM-240718/megatron/core/utils.py
index e4b06b93..e3d63a02 100644
--- a/PAI-Megatron-LM-240718/megatron/core/utils.py
+++ b/PAI-Megatron-LM-240718/megatron/core/utils.py
@@ -466,7 +466,30 @@ def local_multi_tensor_applier(op, noop_flag_buffer, tensor_lists, *args):
 ## computes l2 norm for a list of contiguous tensors
 ## works as a drop-in replacement for amp_C.multi_tensor_l2norm
 def local_multi_tensor_l2_norm(chunk_size, noop_flag, tensor_lists, per_tensor, *args):
-    l2 = [[(torch.norm(tensor)) for tensor in tensor_list] for tensor_list in tensor_lists]
+    # l2 = [[(torch.norm(tensor)) for tensor in tensor_list] for tensor_list in tensor_lists]
+    # l2_reduced = torch.norm(torch.tensor(l2))
+    # l2_cuda = torch.tensor([float(l2_reduced)], dtype=torch.float, device='cuda')
+    # return l2_cuda, None
+
+    tensor_sizes = [((x, y), tensor_lists[x][y].numel()) for x in range(len(tensor_lists)) for y in range(len(tensor_lists[x]))]
+    tensor_sizes.sort(key=lambda x: -x[1])
+    chunk_size = 65536
+    combined_tensor_list = []
+    cnt = 0
+    to_combine = []
+    for (x, y), numel in tensor_sizes:
+        if numel >= chunk_size:
+            combined_tensor_list.append(tensor_lists[x][y])
+            continue
+        to_combine.append(tensor_lists[x][y].view(-1))
+        cnt += numel
+        if cnt >= chunk_size or len(to_combine) >= 16:
+            combined_tensor_list.append(torch.cat(to_combine))
+            to_combine = []
+            cnt = 0
+    if to_combine:
+        combined_tensor_list.append(torch.cat(to_combine))
+    l2 = [torch.norm(tensor) for tensor in combined_tensor_list]
     l2_reduced = torch.norm(torch.tensor(l2))
     l2_cuda = torch.tensor([float(l2_reduced)], dtype=torch.float, device='cuda')
     return l2_cuda, None
diff --git a/PAI-Megatron-LM-240718/megatron/training/arguments.py b/PAI-Megatron-LM-240718/megatron/training/arguments.py
index 2eeea3d5..ab2a1947 100644
--- a/PAI-Megatron-LM-240718/megatron/training/arguments.py
+++ b/PAI-Megatron-LM-240718/megatron/training/arguments.py
@@ -1412,8 +1412,8 @@ def _add_distributed_args(parser):
     group.add_argument('--no-overlap-p2p-communication', action='store_false',
                        help='overlap pipeline parallel communication with forward and backward chunks',
                        dest='overlap_p2p_comm')
-    group.add_argument('--distributed-backend', default='nccl',
-                       choices=['nccl', 'gloo'],
+    group.add_argument('--distributed-backend', default='sccl',
+                       choices=['nccl', 'gloo', 'sccl', 'scclhost'],
                        help='Which backend to use for distributed training.')
     group.add_argument('--distributed-timeout-minutes', type=int, default=10,
                        help='Timeout minutes for torch.distributed.')
diff --git a/PAI-Megatron-LM-240718/megatron/training/training.py b/PAI-Megatron-LM-240718/megatron/training/training.py
index f8169d54..1e6747f2 100644
--- a/PAI-Megatron-LM-240718/megatron/training/training.py
+++ b/PAI-Megatron-LM-240718/megatron/training/training.py
@@ -644,6 +644,10 @@ def setup_model_and_optimizer(
 
 
 from megatron.core.distributed import ParamAndGradBuffer
+try:
+    from nnmoduletools.module_debugger import print_log
+except ImportError:
+    print_log = print
 
 
 def __get_param_buffer_size(buffer: ParamAndGradBuffer) -> int:
@@ -685,6 +689,8 @@ def train_step(
 
     # Forward pass.
     forward_backward_func = get_forward_backward_func()
+    timestamp = time.time()
+
     losses_reduced = forward_backward_func(
         forward_step_func=forward_step_func,
         data_iterator=data_iterator,
@@ -695,6 +701,8 @@ def train_step(
         decoder_seq_length=args.decoder_seq_length,
         forward_only=False,
     )
+    elapsed_time = time.time() - timestamp
+    print_log(f"Time elapsed for iter {args.curr_iteration} forward_backward_func: {elapsed_time}s")
 
     # Empty unused memory.
     if args.empty_unused_memory_level >= 1:
@@ -708,6 +716,8 @@ def train_step(
         memory_stats_collector.sample_overall_data()
 
     # Update parameters.
+    timestamp = time.time()
+
     timers('optimizer', log_level=1).start(barrier=args.barrier_with_L1_time)
     if hasattr(optimizer, 'chunk_manager') and memory_stats_collector is not None:
         update_successful, grad_norm, num_zeros_in_grad = optimizer.step(
@@ -724,6 +734,9 @@ def train_step(
 
         memory_stats_collector.sample_overall_data()
 
+    elapsed_time = time.time() - timestamp
+    print_log(f"Time elapsed for iter {args.curr_iteration} update parameters: {elapsed_time}s")
+
     # Vision momentum.
     if getattr(args, 'vision_pretraining', False) and args.vision_pretraining_type == "dino":
         unwrapped_model = unwrap_model(model[0])
@@ -996,7 +1009,7 @@ def training_log(
             if torch.distributed.get_rank() == 0:
                 num_microbatches = get_num_microbatches()
                 report_theoretical_memory(args, num_microbatches=num_microbatches, verbose=True)
-            report_memory('(after {} iterations)'.format(iteration))
+            # report_memory('(after {} iterations)'.format(iteration))
             report_memory_flag = False
         timers.log(timers_to_log, normalizer=args.log_interval)
 
diff --git a/PAI-Megatron-LM-240718/megatron/training/utils.py b/PAI-Megatron-LM-240718/megatron/training/utils.py
index 5965d785..685f77c7 100644
--- a/PAI-Megatron-LM-240718/megatron/training/utils.py
+++ b/PAI-Megatron-LM-240718/megatron/training/utils.py
@@ -315,11 +315,11 @@ def get_batch_on_this_tp_rank(data_iterator):
            data = None
 
        batch = {
-           'tokens': data["tokens"].cuda(non_blocking = True),
-           'labels': data["labels"].cuda(non_blocking = True),
-           'loss_mask': data["loss_mask"].cuda(non_blocking = True),
-           'attention_mask': None if "attention_mask" not in data else data["attention_mask"].cuda(non_blocking = True),
-           'position_ids': data["position_ids"].cuda(non_blocking = True)
+           'tokens': data["tokens"].cuda(non_blocking = False),
+           'labels': data["labels"].cuda(non_blocking = False),
+           'loss_mask': data["loss_mask"].cuda(non_blocking = False),
+           'attention_mask': None if "attention_mask" not in data else data["attention_mask"].cuda(non_blocking = False),
+           'position_ids': data["position_ids"].cuda(non_blocking = False)
        }
 
        if args.pipeline_model_parallel_size == 1:
diff --git a/examples/qwen2/pretrain_qwen.py b/examples/qwen2/pretrain_qwen.py
index 8ab6be0..105638f 100644
--- a/examples/qwen2/pretrain_qwen.py
+++ b/examples/qwen2/pretrain_qwen.py
@@ -6,7 +6,19 @@ from functools import partial
 from typing import Union
 
 import torch
-import torch._dynamo
+# import torch._dynamo
+try:
+    import torch_tpu
+    from torch_tpu import accelerator
+except ImportError:
+    import sys
+    sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", "..", "tpu-train", "torch_tpu"))
+    import accelerator
+
+from distutils.util import strtobool
+if strtobool(os.environ.get('ENABLE_FRAMEWORK_DEBUGGER', "0")):
+    from nnmoduletools.module_debugger import register_hook
+
 from megatron.core import mpu
 from megatron.core.datasets.blended_megatron_dataset_builder import (
     BlendedMegatronDatasetBuilder,
@@ -37,7 +49,7 @@ from megatron_patch.model.qwen2.model import GPTModel
 from megatron_patch.model.qwen2.transformer_config import Qwen2TransformerConfig
 from megatron_patch.tokenizer import build_tokenizer, get_tokenizer
 
-torch._dynamo.config.suppress_errors = True
+# torch._dynamo.config.suppress_errors = True
 
 
 def model_provider(
@@ -78,6 +90,9 @@ def model_provider(
         seq_len_interpolation_factor=args.rotary_seq_len_interpolation_factor,
     )
 
+    if strtobool(os.environ.get('ENABLE_FRAMEWORK_DEBUGGER', "0")):
+        model.apply(register_hook)
+
     return model
 
 
diff --git a/megatron_patch/model/qwen2/layer_specs.py b/megatron_patch/model/qwen2/layer_specs.py
index 68a76dd..70874e1 100755
--- a/megatron_patch/model/qwen2/layer_specs.py
+++ b/megatron_patch/model/qwen2/layer_specs.py
@@ -16,13 +16,13 @@ from megatron.core.fusions.fused_bias_dropout import get_bias_dropout_add
 from megatron.core.fusions.fused_layer_norm import FusedLayerNorm
 from megatron.core.tensor_parallel.layers import ColumnParallelLinear, RowParallelLinear
 
-from megatron.core.transformer.custom_layers.transformer_engine import (
-    TEDotProductAttention,
-    TELayerNormColumnParallelLinear,
-    TENorm,
-    TERowParallelLinear,
-    TEColumnParallelLinear,
-)
+# from megatron.core.transformer.custom_layers.transformer_engine import (
+#     TEDotProductAttention,
+#     TELayerNormColumnParallelLinear,
+#     TENorm,
+#     TERowParallelLinear,
+#     TEColumnParallelLinear,
+# )
 from megatron.core.transformer.dot_product_attention import DotProductAttention
 from megatron.core.transformer.enums import AttnMaskType
 from megatron.core.transformer.identity_op import IdentityOp
diff --git a/megatron_patch/model/qwen2/transformer/attention.py b/megatron_patch/model/qwen2/transformer/attention.py
index 365ce2a..e8881ea 100644
--- a/megatron_patch/model/qwen2/transformer/attention.py
+++ b/megatron_patch/model/qwen2/transformer/attention.py
@@ -26,7 +26,7 @@ from megatron.core.parallel_state import (
     get_tensor_model_parallel_rank,
     get_tensor_model_parallel_world_size,
 )
-from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim
+from megatron.core.transformer.attention import SplitAlongDim
 from megatron.core.transformer.module import MegatronModule
 from megatron.core.transformer.spec_utils import ModuleSpec, build_module
 from megatron.core.transformer.transformer_config import TransformerConfig
diff --git a/megatron_patch/model/qwen2/transformer_block.py b/megatron_patch/model/qwen2/transformer_block.py
index fb4745a..3b3fd52 100755
--- a/megatron_patch/model/qwen2/transformer_block.py
+++ b/megatron_patch/model/qwen2/transformer_block.py
@@ -23,12 +23,12 @@ from megatron.core.dist_checkpointing.mapping import ShardedStateDict
 from megatron.core.dist_checkpointing.utils import replace_prefix_for_sharding
 from megatron.core.fusions.fused_layer_norm import FusedLayerNorm
 from megatron.core.packed_seq_params import PackedSeqParams
-from megatron.core.transformer.custom_layers.transformer_engine import (
-    TEDelayedScaling,
-    TENorm,
-    get_cpu_offload_context,
-    te_checkpoint,
-)
+# from megatron.core.transformer.custom_layers.transformer_engine import (
+#     TEDelayedScaling,
+#     TENorm,
+#     get_cpu_offload_context,
+#     te_checkpoint,
+# )
 from megatron.core.transformer.enums import AttnMaskType
 from megatron.core.transformer.module import MegatronModule
 from megatron.core.transformer.spec_utils import ModuleSpec, build_module
@@ -130,26 +130,26 @@ class TransformerBlock(MegatronModule):
 
         self.checkpoint_core_attention = self.config.recompute_granularity == 'selective'
 
-        if get_cpu_offload_context is not None:
-            (
-                self.offload_context,
-                self.group_prefetch_offload_commit_async,
-            ) = get_cpu_offload_context(
-                self.config.cpu_offloading,
-                self.config.cpu_offloading_num_layers,
-                self.config.cpu_offloading_activations,
-                self.config.cpu_offloading_weights,
-            )
-            self.config._cpu_offloading_context = (
-                self.offload_context if self.config.cpu_offloading else None
-            )
-        else:
-            assert (
-                self.config.cpu_offloading == False
-            ), "CPU Offloading is enabled when TE is not present"
+        # if get_cpu_offload_context is not None:
+        #     (
+        #         self.offload_context,
+        #         self.group_prefetch_offload_commit_async,
+        #     ) = get_cpu_offload_context(
+        #         self.config.cpu_offloading,
+        #         self.config.cpu_offloading_num_layers,
+        #         self.config.cpu_offloading_activations,
+        #         self.config.cpu_offloading_weights,
+        #     )
+        #     self.config._cpu_offloading_context = (
+        #         self.offload_context if self.config.cpu_offloading else None
+        #     )
+        # else:
+        assert (
+            self.config.cpu_offloading == False
+        ), "CPU Offloading is enabled when TE is not present"
 
-            self.offload_context, self.group_prefetch_offload_commit_async = nullcontext(), None
-            self.config._cpu_offloading_context = None
+        self.offload_context, self.group_prefetch_offload_commit_async = nullcontext(), None
+        self.config._cpu_offloading_context = None
 
         self._build_layers()
         self.num_layers_per_pipeline_rank = len(self.layers)
diff --git a/toolkits/model_checkpoints_convertor/qwen/hf2mcore_qwen2_convertor.sh b/toolkits/model_checkpoints_convertor/qwen/hf2mcore_qwen2_convertor.sh
index 9921466..84f7d26 100644
--- a/toolkits/model_checkpoints_convertor/qwen/hf2mcore_qwen2_convertor.sh
+++ b/toolkits/model_checkpoints_convertor/qwen/hf2mcore_qwen2_convertor.sh
@@ -2,7 +2,7 @@
 set -e
 export CUDA_VISIBLE_DEVICES=7
 START_TIME=$SECONDS
-MASTER_ADDR=localhost
+MASTER_ADDR=127.0.0.1
 MASTER_PORT=$(shuf -n 1 -i 10000-65535)
 
 MODEL_SIZE=$1
@@ -18,7 +18,7 @@ HF_CKPT_PATH=${10}
 
 CURRENT_DIR="$( cd "$( dirname "$0" )" && pwd )"
 MEGATRON_PATH=$( dirname $(dirname $( dirname ${CURRENT_DIR})))
-export PYTHONPATH=$PYTHONPATH:${MEGATRON_PATH}:${MEGATRON_PATH}/Megatron-LM-240612
+export PYTHONPATH=$PYTHONPATH:${MEGATRON_PATH}:${MEGATRON_PATH}/Megatron-LM-240718
 
 
 if [ $MODEL_SIZE = 0.5B ]; then
@@ -158,11 +158,17 @@ if [ $PR = fp16 ]; then
 elif [ $PR = bf16 ]; then
     pr_options=" \
         --bf16"
-
+    echo "PR = bf16"
 fi
 
+# extra_options=" \
+#         --distributed-backend ${BACKEND}"
+fuse_options=" \
+        --no-gradient-accumulation-fusion \
+        --no-bias-gelu-fusion \
+        --no-bias-dropout-fusion"
 
-DISTRIBUTED_ARGS="--nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr $MASTER_ADDR --master_port $MASTER_PORT"
+DISTRIBUTED_ARGS="--nproc_per_node 2 --nnodes 1 --master_addr $MASTER_ADDR --master_port 7000"
 
 torchrun ${DISTRIBUTED_ARGS} hf2mcore_qwen2_dense_and_moe_gqa.py \
     --load ${SOURCE_CKPT_PATH} \
@@ -177,7 +183,7 @@ torchrun ${DISTRIBUTED_ARGS} hf2mcore_qwen2_dense_and_moe_gqa.py \
     --ffn-hidden-size ${INTERMEDIATE_SIZE} \
     --num-attention-heads ${NUM_ATTENTION_HEADS} \
     --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \
-    --seq-length 1 \
+    --seq-length 512 \
     --no-async-tensor-model-parallel-allreduce \
     --patch-tokenizer-type Qwen2Tokenizer \
     --extra-vocab-size ${EXTRA_VOCAB_SIZE} \
@@ -199,7 +205,8 @@ torchrun ${DISTRIBUTED_ARGS} hf2mcore_qwen2_dense_and_moe_gqa.py \
     ${te_options} \
     ${convert_options} \
     ${pr_options} \
-    ${cpu_options}
+    ${cpu_options} \
+    ${fuse_options}
 
 
 ELAPSED_TIME=$(($SECONDS - $START_TIME))
diff --git a/toolkits/model_checkpoints_convertor/qwen/hf2mcore_qwen2_dense_and_moe_gqa.py b/toolkits/model_checkpoints_convertor/qwen/hf2mcore_qwen2_dense_and_moe_gqa.py
index 1e72483..ef749a6 100644
--- a/toolkits/model_checkpoints_convertor/qwen/hf2mcore_qwen2_dense_and_moe_gqa.py
+++ b/toolkits/model_checkpoints_convertor/qwen/hf2mcore_qwen2_dense_and_moe_gqa.py
@@ -11,7 +11,17 @@ from transformers import (
     AutoTokenizer,
 )
 
-from transformers.modeling_utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME, shard_checkpoint, load_sharded_checkpoint
+import torch
+# import torch._dynamo
+try:
+    import torch_tpu
+    from torch_tpu import accelerator
+except ImportError:
+    import sys
+    sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", "..", "tpu-train", "torch_tpu"))
+    import accelerator
+
+from transformers.modeling_utils import SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME, shard_checkpoint, load_sharded_checkpoint
 from megatron.training.initialize import initialize_megatron
 from megatron.training import get_args
 from megatron.training.checkpointing import get_checkpoint_name, get_checkpoint_tracker_filename, read_metadata
@@ -24,12 +34,12 @@ sys.path.append(os.path.join(path_dir, "examples"))
 from qwen2.pretrain_qwen import model_provider
 from megatron_patch.arguments import get_patch_args
 
-torch.backends.cudnn.deterministic = True
-torch.backends.cudnn.benchmark = False
-torch.backends.cuda.matmul.allow_tf32 = False
-torch.backends.cudnn.allow_tf32 = False
-torch.backends.cuda.enable_mem_efficient_sdp(False)
-torch.backends.cuda.enable_flash_sdp(False)
+# torch.backends.cudnn.deterministic = True
+# torch.backends.cudnn.benchmark = False
+# torch.backends.cuda.matmul.allow_tf32 = False
+# torch.backends.cudnn.allow_tf32 = False
+# torch.backends.cuda.enable_mem_efficient_sdp(False)
+# torch.backends.cuda.enable_flash_sdp(False)
 
 def add_model_args(parser):
 
@@ -752,6 +762,10 @@ def save_hfmodel(args, model):
     os.makedirs(args.save, exist_ok=True)
     for shard_file, shard in shards.items():
         if args.save_safetensors:
+            for key, value in index["weight_map"].items():
+                new_value = value.replace("pytorch_", "")
+                new_value = new_value.replace(".bin", ".safetensors")
+                index["weight_map"][key] = new_value
             shard_file = shard_file.replace("pytorch_", "")
             shard_file = shard_file.replace(".bin", ".safetensors")
             target_file = os.path.join(args.save, shard_file)
@@ -766,9 +780,9 @@ def save_hfmodel(args, model):
             torch.save(shard, target_file)
 
     if index is None:
-        print(f"Model weights saved in {os.path.join(args.save, WEIGHTS_NAME)}")
+        print(f"Model weights saved in {os.path.join(args.save, SAFE_WEIGHTS_NAME)}")
     else:
-        save_index_file = os.path.join(args.save, WEIGHTS_INDEX_NAME)
+        save_index_file = os.path.join(args.save, SAFE_WEIGHTS_INDEX_NAME)
         # Save the index as well
         with open(save_index_file, "w", encoding="utf-8") as f:
             content = json.dumps(index, indent=2, sort_keys=True) + "\n"
